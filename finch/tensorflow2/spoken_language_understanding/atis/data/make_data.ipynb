{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"make_data.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"aWjiMM8qbM__","colab_type":"code","outputId":"65bdd0f2-f4d6-4fc8-9e4f-474bc332410e","executionInfo":{"status":"ok","timestamp":1553671983589,"user_tz":-480,"elapsed":2152,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["\n","\"\"\"\n","We use following lines because we are running on Google Colab\n","If you are running notebook on a local computer, you don't need this cell\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir('/content/gdrive/My Drive/finch/tensorflow2/spoken_language_understanding/atis/data')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"Kw81Prcg8zRl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":367},"outputId":"81116b1f-2aec-425b-ba9e-36a0f86a7ca7","executionInfo":{"status":"ok","timestamp":1553671988335,"user_tz":-480,"elapsed":6869,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}}},"cell_type":"code","source":["!pip install tf-nightly-2.0-preview"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tf-nightly-2.0-preview in /usr/local/lib/python3.6/dist-packages (2.0.0.dev20190326)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.9)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.11.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.7)\n","Requirement already satisfied: tensorflow-estimator-2.0-preview in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.14.0.dev2019032600)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.33.1)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.2)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.7.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.1)\n","Requirement already satisfied: tb-nightly<1.15.0a0,>=1.14.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.14.0a20190319)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.14.6)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.15.0)\n","Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.1.4)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-2.0-preview) (2.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview) (40.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-2.0-preview) (3.0.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-2.0-preview) (0.14.1)\n"],"name":"stdout"}]},{"metadata":{"id":"sRS_9RVFjoe1","colab_type":"code","colab":{}},"cell_type":"code","source":["from pathlib import Path\n","from collections import Counter\n","\n","import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WszANU_obncn","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","Make Vocabulary (Words, Intents and Slots)\n","\"\"\"\n","\n","Path('../vocab').mkdir(exist_ok=True)\n","\n","counter_word = Counter()\n","counter_intent = Counter()\n","counter_slot = Counter()\n","\n","with open('../data/atis.train.w-intent.iob') as f:\n","  for line in f:\n","    line = line.rstrip()\n","    text, slot_intent = line.split('\\t')\n","    words = text.split()[1:-1]\n","    words = ['<digit>' if str.isdigit(w) else w for w in words]\n","    slot_intent = slot_intent.split()\n","    slots, intent = slot_intent[1:-1], slot_intent[-1]\n","    assert len(words) == len(slots)\n","    \n","    counter_word.update(words)\n","    counter_intent.update([intent])\n","    counter_slot.update(slots)\n","\n","most_common = lambda x: [w for w, freq in x.most_common()]\n","\n","words = ['<pad>'] + most_common(counter_word)\n","intents  = most_common(counter_intent)\n","slots  = most_common(counter_slot)\n","\n","for vocab_li, path in zip(\n","    [words, intents, slots],\n","    ['../vocab/word.txt', '../vocab/intent.txt', '../vocab/slot.txt']):\n","  with open(path, 'w') as f:\n","    for w in vocab_li:\n","      f.write(w+'\\n')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hiuGH-P5ZkjM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":433},"outputId":"d736ed35-19d6-4c3b-f559-b6c19aeb4ca6","executionInfo":{"status":"ok","timestamp":1553672204733,"user_tz":-480,"elapsed":223211,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}}},"cell_type":"code","source":["\"\"\"\n","Make Pretrained Embedding\n","\"\"\"\n","word2idx = {}\n","with open('../vocab/word.txt') as f:\n","  for i, line in enumerate(f):\n","    line = line.rstrip()\n","    word2idx[line] = i\n","    \n","embedding = np.zeros((len(word2idx)+1, 300)) # + 1 for unknown word\n","\n","with open('../data/glove.840B.300d.txt') as f:\n","  count = 0\n","  for i, line in enumerate(f):\n","    if i % 100000 == 0:\n","      print('- At line {}'.format(i))\n","    line = line.rstrip()\n","    sp = line.split(' ')\n","    word, vec = sp[0], sp[1:]\n","    if word in word2idx:\n","      count += 1\n","      embedding[word2idx[word]] = np.asarray(vec, dtype='float32')\n","      \n","print(\"[%d / %d] words have found pre-trained values\"%(count, len(word2idx)))\n","np.save('../vocab/word.npy', embedding)\n","print('Saved ../vocab/word.npy')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["- At line 0\n","- At line 100000\n","- At line 200000\n","- At line 300000\n","- At line 400000\n","- At line 500000\n","- At line 600000\n","- At line 700000\n","- At line 800000\n","- At line 900000\n","- At line 1000000\n","- At line 1100000\n","- At line 1200000\n","- At line 1300000\n","- At line 1400000\n","- At line 1500000\n","- At line 1600000\n","- At line 1700000\n","- At line 1800000\n","- At line 1900000\n","- At line 2000000\n","- At line 2100000\n","[725 / 749] words have found pre-trained values\n","Saved ../vocab/word.npy\n"],"name":"stdout"}]}]}