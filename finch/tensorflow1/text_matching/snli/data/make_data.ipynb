{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "make_data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "iGotAVAyOk3M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We use following lines because we are running on Google Colab\n",
        "If you are running notebook on a local computer, you don't need this cell\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/finch/tensorflow1/text_matching/snli/main')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vHncQmfEJ30C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from collections import Counter\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bzKrUJt8JsBm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Make Data"
      ]
    },
    {
      "metadata": {
        "id": "T82HdcN2Vsib",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize(x):\n",
        "  x = x.lower()\n",
        "  x = x.replace('.', '')\n",
        "  x = x.replace(',', '')\n",
        "  x = x.replace(';', '')\n",
        "  x = x.replace('!', '')\n",
        "  x = x.replace('#', '')\n",
        "  x = x.replace('(', '')\n",
        "  x = x.replace(')', '')\n",
        "  x = x.replace(':', '')\n",
        "  x = x.replace('%', '')\n",
        "  x = x.replace('&', '')\n",
        "  x = x.replace('$', '')\n",
        "  x = x.replace('?', '')\n",
        "  x = x.replace('\"', '')\n",
        "  x = x.replace('/', ' ')\n",
        "  x = x.replace('-', ' ')\n",
        "  x = x.replace(\"n't\", \" n't \")\n",
        "  x = x.replace(\"'\", \" ' \")\n",
        "  x = re.sub(r'\\d+', ' <num> ', x)\n",
        "  x = re.sub(r'\\s+', ' ', x)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u6y8_wZrPa2v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def write_text(in_path, out_path):\n",
        "  with open(in_path) as f_in, open(out_path, 'w') as f_out:\n",
        "    f_in.readline()\n",
        "    for line in f_in:\n",
        "      line = line.rstrip()\n",
        "      sp = line.split('\\t')\n",
        "      label, sent1, sent2 = sp[0], sp[5], sp[6]\n",
        "\n",
        "      sent1 = normalize(sent1)\n",
        "      sent2 = normalize(sent2)\n",
        "\n",
        "      f_out.write(label+'\\t'+sent1+'\\t'+sent2+'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYdt2O6DXMgX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "write_text('../data/snli_1.0/snli_1.0_train.txt', '../data/train.txt')\n",
        "write_text('../data/snli_1.0/snli_1.0_test.txt', '../data/test.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-fUXXTPlJuan",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Make Vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "gjYwDn7MJt8Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "counter = Counter()\n",
        "with open('../data/train.txt') as f:\n",
        "  for line in f:\n",
        "    line = line.rstrip()\n",
        "    label, sent1, sent2 = line.split('\\t')\n",
        "    counter.update(sent1.split())\n",
        "    counter.update(sent2.split())\n",
        "\n",
        "words = [w for w, freq in counter.most_common() if freq >= 3]\n",
        "\n",
        "Path('../vocab').mkdir(exist_ok=True)\n",
        "\n",
        "with open('../vocab/word.txt', 'w') as f:\n",
        "  f.write('<pad>'+'\\n')\n",
        "  for w in words:\n",
        "    f.write(w+'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "74s9jgIhJ7JE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Make Pretrained Embedding"
      ]
    },
    {
      "metadata": {
        "id": "XP8-nshCjJPB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def norm_weight(nin, nout, scale=0.01):\n",
        "  W = scale * np.random.randn(nin, nout)\n",
        "  return W.astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NncSe-MWJ_rO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word2idx = {}\n",
        "with open('../vocab/word.txt') as f:\n",
        "  for i, line in enumerate(f):\n",
        "    line = line.rstrip()\n",
        "    word2idx[line] = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y6zCLps-KF5l",
        "colab_type": "code",
        "outputId": "e184d4db-4ed1-4394-8cb9-8ea406d23f8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "cell_type": "code",
      "source": [
        "embedding = norm_weight(len(word2idx)+1, 300)\n",
        "\n",
        "with open('../data/glove.840B.300d.txt') as f:\n",
        "  count = 0\n",
        "  for i, line in enumerate(f):\n",
        "    if i % 100000 == 0:\n",
        "      print('- At line {}'.format(i))\n",
        "    line = line.rstrip()\n",
        "    sp = line.split(' ')\n",
        "    word, vec = sp[0], sp[1:]\n",
        "    if word in word2idx:\n",
        "      count += 1\n",
        "      embedding[word2idx[word]] = np.asarray(vec, dtype=np.float32)\n",
        "      \n",
        "print(\"[%d / %d] words have found pre-trained values\"%(count, len(word2idx)))\n",
        "np.save('../vocab/word.npy', embedding)\n",
        "print('Saved ../vocab/word.npy')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- At line 0\n",
            "- At line 100000\n",
            "- At line 200000\n",
            "- At line 300000\n",
            "- At line 400000\n",
            "- At line 500000\n",
            "- At line 600000\n",
            "- At line 700000\n",
            "- At line 800000\n",
            "- At line 900000\n",
            "- At line 1000000\n",
            "- At line 1100000\n",
            "- At line 1200000\n",
            "- At line 1300000\n",
            "- At line 1400000\n",
            "- At line 1500000\n",
            "- At line 1600000\n",
            "- At line 1700000\n",
            "- At line 1800000\n",
            "- At line 1900000\n",
            "- At line 2000000\n",
            "- At line 2100000\n",
            "[20333 / 20883] words have found pre-trained values\n",
            "Saved ../vocab/word.npy\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}