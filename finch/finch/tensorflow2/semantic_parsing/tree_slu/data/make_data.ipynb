{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"make_data.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"2nf6RHAUhdCa","colab_type":"code","outputId":"a6815836-2b38-4f84-8a3a-5cf6dac79589","executionInfo":{"status":"ok","timestamp":1556769613805,"user_tz":-480,"elapsed":987,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["\"\"\"\n","We use following lines because we are running on Google Colab\n","If you are running notebook on a local computer, you don't need this cell\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir('/content/gdrive/My Drive/finch/tensorflow2/semantic_parsing/tree_slu/data')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"bqY4ftJIi7pP","colab_type":"code","colab":{}},"cell_type":"code","source":["from pathlib import Path\n","from collections import Counter"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xN_5InWziJsQ","colab_type":"code","colab":{}},"cell_type":"code","source":["Path('../vocab').mkdir(exist_ok=True)\n","enc_counter = Counter()\n","dec_counter = Counter()\n","\n","with open('../data/train.tsv') as f:\n","  for line in f:\n","    line = line.rstrip()\n","    text_raw, text_tokenized, label = line.split('\\t')\n","    enc_counter.update(text_tokenized.lower().split())\n","    dec_counter.update(label.replace('[', '[ ').lower().split())\n","\n","with open('../vocab/source.txt', 'w') as f:\n","  f.write('<pad>\\n')\n","  for (w, freq) in enc_counter.most_common():\n","    f.write(w+'\\n')\n","    \n","with open('../vocab/target.txt', 'w') as f:\n","  f.write('<pad>\\n')\n","  f.write('<start>\\n')\n","  f.write('<end>\\n')\n","  for (w, freq) in dec_counter.most_common():\n","    f.write(w+'\\n')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IbtuV4gtsOFs","colab_type":"text"},"cell_type":"markdown","source":["Make Pretrained Embedding"]},{"metadata":{"id":"K7EjclpRsCGO","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","word2idx = {}\n","with open('../vocab/target.txt') as f:\n","  for i, line in enumerate(f):\n","    line = line.rstrip()\n","    word2idx[line] = i"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GsJe_2-vsRvD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"outputId":"5ee9a2a0-f5d7-4c0d-9692-9c8e7ec1e13f","executionInfo":{"status":"ok","timestamp":1556769799816,"user_tz":-480,"elapsed":186952,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}}},"cell_type":"code","source":["embedding = np.zeros((len(word2idx)+1, 300)) # + 1 for unknown word\n","\n","with open('../data/glove.840B.300d.txt') as f:\n","  count = 0\n","  for i, line in enumerate(f):\n","    if i % 100000 == 0:\n","      print('- At line {}'.format(i))\n","    line = line.rstrip()\n","    sp = line.split(' ')\n","    word, vec = sp[0], sp[1:]\n","    if word in word2idx:\n","      count += 1\n","      embedding[word2idx[word]] = np.asarray(vec, dtype='float32')\n","      \n","print(\"[%d / %d] words have found pre-trained values\"%(count, len(word2idx)))\n","np.save('../vocab/word.npy', embedding)\n","print('Saved ../vocab/word.npy')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["- At line 0\n","- At line 100000\n","- At line 200000\n","- At line 300000\n","- At line 400000\n","- At line 500000\n","- At line 600000\n","- At line 700000\n","- At line 800000\n","- At line 900000\n","- At line 1000000\n","- At line 1100000\n","- At line 1200000\n","- At line 1300000\n","- At line 1400000\n","- At line 1500000\n","- At line 1600000\n","- At line 1700000\n","- At line 1800000\n","- At line 1900000\n","- At line 2000000\n","- At line 2100000\n","[8078 / 8691] words have found pre-trained values\n","Saved ../vocab/word.npy\n"],"name":"stdout"}]}]}