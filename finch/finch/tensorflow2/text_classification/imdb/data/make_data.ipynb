{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"make_data.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"n1ZkwW_zl_1i","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","We use following lines because we are running on Google Colab\n","If you are running notebook on a local computer, you don't need these\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir('/content/gdrive/My Drive/finch/tensorflow2/text_classification/imdb/data')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wh-7XLHuLcir","colab_type":"code","outputId":"ec28b39c-3d79-4ca4-86eb-041f6a3a1013","executionInfo":{"status":"ok","timestamp":1551929307375,"user_tz":-480,"elapsed":130117,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":540}},"cell_type":"code","source":["!pip install tf-nightly-2.0-preview"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting tf-nightly-2.0-preview\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/49/3e023e9b87c0c2e403b36c1aced3e9ab1a750da81456ca0cf2bafd4ff09c/tf_nightly_2.0_preview-2.0.0.dev20190306-cp36-cp36m-manylinux1_x86_64.whl (79.7MB)\n","\u001b[K    100% |████████████████████████████████| 79.7MB 348kB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.11.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.6.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n","Collecting google-pasta>=0.1.2 (from tf-nightly-2.0-preview)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/96/adbd4eafe72ce9b5ca6f168fbf109386e1b601f7c59926a11e9d7b7a5b44/google_pasta-0.1.4-py3-none-any.whl (51kB)\n","\u001b[K    100% |████████████████████████████████| 61kB 22.5MB/s \n","\u001b[?25hCollecting tensorflow-estimator-2.0-preview (from tf-nightly-2.0-preview)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/e0/2a8105005a9f250e46317e582f87a5175a25c63832dfd67e4ca15d0659bd/tensorflow_estimator_2.0_preview-1.14.0.dev2019030600-py2.py3-none-any.whl (351kB)\n","\u001b[K    100% |████████████████████████████████| 358kB 21.0MB/s \n","\u001b[?25hCollecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf-nightly-2.0-preview)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/5e/79a66b54ddec968c9acd648a63213b236a7bb9348979944c234c8959da4d/tb_nightly-1.14.0a20190306-py3-none-any.whl (3.0MB)\n","\u001b[K    100% |████████████████████████████████| 3.0MB 11.7MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.9)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.2)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.33.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.15.0)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.14.6)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.1)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview) (40.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-2.0-preview) (0.14.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-2.0-preview) (3.0.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-2.0-preview) (2.8.0)\n","Installing collected packages: google-pasta, tensorflow-estimator-2.0-preview, tb-nightly, tf-nightly-2.0-preview\n","Successfully installed google-pasta-0.1.4 tb-nightly-1.14.0a20190306 tensorflow-estimator-2.0-preview-1.14.0.dev2019030600 tf-nightly-2.0-preview-2.0.0.dev20190306\n"],"name":"stdout"}]},{"metadata":{"id":"R6JhV0pNnCbr","colab_type":"code","outputId":"17e39db5-4691-4003-f1b7-5b195e0c71bc","executionInfo":{"status":"ok","timestamp":1551929309538,"user_tz":-480,"elapsed":132258,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","from collections import Counter\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","print('TensorFlow Version:', tf.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow Version: 2.0.0-dev20190306\n"],"name":"stdout"}]},{"metadata":{"id":"tB8fEYVSJQ4C","colab_type":"text"},"cell_type":"markdown","source":["Make Data"]},{"metadata":{"id":"uHJL0ukkmW4O","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","sort texts (and labels) according to the length of text\n","\"\"\"\n","def sort_by_len(x, y):\n","    x, y = np.asarray(x), np.asarray(y)\n","    idx = sorted(range(len(x)), key=lambda i: len(x[i]))\n","    return x[idx], y[idx]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BepBfbq6nNik","colab_type":"code","outputId":"9a7ef4e3-ebe7-4d4a-8f57-bbd2f0754d1d","executionInfo":{"status":"ok","timestamp":1551929309549,"user_tz":-480,"elapsed":132264,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["_word2idx = tf.keras.datasets.imdb.get_word_index()\n","word2idx = {w: i+3 for w, i in _word2idx.items()}\n","word2idx['<pad>'] = 0\n","word2idx['<start>'] = 1\n","word2idx['<unk>'] = 2\n","idx2word = {i: w for w, i in word2idx.items()}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n","1646592/1641221 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"metadata":{"id":"DR84j5-FpLTj","colab_type":"code","outputId":"e121b02c-f989-455f-be24-33fd5799d446","executionInfo":{"status":"ok","timestamp":1551929320039,"user_tz":-480,"elapsed":142752,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()\n","\n","x_train, y_train = sort_by_len(x_train, y_train)\n","x_test, y_test = sort_by_len(x_test, y_test)\n","\n","def write_file(f_path, xs, ys):\n","  with open(f_path, 'w') as f:\n","      for x, y in zip(xs, ys):\n","          f.write(str(y)+'\\t'+' '.join([idx2word[i] for i in x][1:])+'\\n')\n","\n","write_file('../data/train.txt', x_train, y_train)\n","write_file('../data/test.txt', x_test, y_test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"metadata":{"id":"_TteqsURJEPG","colab_type":"text"},"cell_type":"markdown","source":["Make Vocabulary"]},{"metadata":{"id":"aXE-HaQ6IPb2","colab_type":"code","outputId":"82f4d15a-d32d-48d1-a03e-07ff44633586","executionInfo":{"status":"ok","timestamp":1551929321792,"user_tz":-480,"elapsed":144480,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["counter = Counter()\n","with open('../data/train.txt') as f:\n","  for line in f:\n","    line = line.rstrip()\n","    label, words = line.split('\\t')\n","    words = words.split(' ')\n","    counter.update(words)\n","\n","words = ['<pad>'] + [w for w, freq in counter.most_common() if freq >= 10]\n","print('Vocab Size:', len(words))\n","\n","Path('../vocab').mkdir(exist_ok=True)\n","\n","with open('../vocab/word.txt', 'w') as f:\n","  for w in words:\n","    f.write(w+'\\n')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Vocab Size: 20598\n"],"name":"stdout"}]},{"metadata":{"id":"cbZTiO8ZLx3j","colab_type":"text"},"cell_type":"markdown","source":["Make Pretrained Embedding"]},{"metadata":{"id":"4-dA_qsVLsvd","colab_type":"code","colab":{}},"cell_type":"code","source":["word2idx = {}\n","with open('../vocab/word.txt') as f:\n","  for i, line in enumerate(f):\n","    line = line.rstrip()\n","    word2idx[line] = i"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qgZ8JT59L20j","colab_type":"code","outputId":"ccb308a7-55ec-43e2-cdc1-0b929bb3eef9","executionInfo":{"status":"ok","timestamp":1551929513564,"user_tz":-480,"elapsed":336247,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":433}},"cell_type":"code","source":["embedding = np.zeros((len(word2idx)+1, 300)) # + 1 for unknown word\n","\n","with open('../data/glove.840B.300d.txt') as f:\n","  count = 0\n","  for i, line in enumerate(f):\n","    if i % 100000 == 0:\n","      print('- At line {}'.format(i))\n","    line = line.rstrip()\n","    sp = line.split(' ')\n","    word, vec = sp[0], sp[1:]\n","    if word in word2idx:\n","      count += 1\n","      embedding[word2idx[word]] = np.asarray(vec, dtype='float32')\n","      \n","print(\"[%d / %d] words have found pre-trained values\"%(count, len(word2idx)))\n","np.save('../vocab/word.npy', embedding)\n","print('Saved ../vocab/word.npy')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["- At line 0\n","- At line 100000\n","- At line 200000\n","- At line 300000\n","- At line 400000\n","- At line 500000\n","- At line 600000\n","- At line 700000\n","- At line 800000\n","- At line 900000\n","- At line 1000000\n","- At line 1100000\n","- At line 1200000\n","- At line 1300000\n","- At line 1400000\n","- At line 1500000\n","- At line 1600000\n","- At line 1700000\n","- At line 1800000\n","- At line 1900000\n","- At line 2000000\n","- At line 2100000\n","[19487 / 20598] words have found pre-trained values\n","Saved ../vocab/word.npy\n"],"name":"stdout"}]}]}