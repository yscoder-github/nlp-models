{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tfidf_lr.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"EJNFOgo8NuK5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":297},"outputId":"4881d99a-f160-4c00-c2c7-edc93e48ab39","executionInfo":{"status":"ok","timestamp":1555652859203,"user_tz":-480,"elapsed":90714,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}}},"cell_type":"code","source":["!apt-get install openjdk-8-jdk-headless > /dev/null\n","!wget http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz\n","!tar xf spark-2.4.1-bin-hadoop2.7.tgz\n","!pip install findspark"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2019-04-19 05:46:15--  http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz\n","Resolving mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)... 101.6.8.193, 2402:f000:1:408:8100::1\n","Connecting to mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)|101.6.8.193|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 230778742 (220M) [application/octet-stream]\n","Saving to: ‘spark-2.4.1-bin-hadoop2.7.tgz’\n","\n","spark-2.4.1-bin-had 100%[===================>] 220.09M  3.69MB/s    in 70s     \n","\n","2019-04-19 05:47:26 (3.13 MB/s) - ‘spark-2.4.1-bin-hadoop2.7.tgz’ saved [230778742/230778742]\n","\n","Collecting findspark\n","  Downloading https://files.pythonhosted.org/packages/b1/c8/e6e1f6a303ae5122dc28d131b5a67c5eb87cbf8f7ac5b9f87764ea1b1e1e/findspark-1.3.0-py2.py3-none-any.whl\n","Installing collected packages: findspark\n","Successfully installed findspark-1.3.0\n"],"name":"stdout"}]},{"metadata":{"id":"_owq4Fy85YUL","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","We are running these lines because we are operating on Google Colab\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.1-bin-hadoop2.7\"\n","os.chdir('/content/gdrive/My Drive/finch/spark/text_classification/imdb')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"v8CmbbII_qvZ","colab_type":"code","colab":{}},"cell_type":"code","source":["import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","from pyspark.ml import Pipeline\n","from pyspark.ml.feature import CountVectorizer, IDF\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yiC5WoSYo7qR","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_idx2word(_index_from=3):\n","  word2idx = tf.keras.datasets.imdb.get_word_index()\n","  word2idx = {k:(v+_index_from) for k,v in word2idx.items()}\n","  word2idx[\"<pad>\"] = 0\n","  word2idx[\"<start>\"] = 1\n","  word2idx[\"<unk>\"] = 2\n","  idx2word = {idx: w for w, idx in word2idx.items()}\n","  return idx2word\n","\n","\n","def make_df(x, y):\n","  return sess.createDataFrame(\n","    [(int(y_), [idx2word[idx] for idx in x_]) for x_, y_ in zip(x, y)],\n","    ['label', 'words'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FkzFLHIYo9z5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"c58076e5-9478-4a4e-ff2e-b9445dd651b6","executionInfo":{"status":"ok","timestamp":1555652895708,"user_tz":-480,"elapsed":127146,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}}},"cell_type":"code","source":["import tensorflow as tf\n","idx2word = get_idx2word()\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=20000)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n","1646592/1641221 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"metadata":{"id":"8uJ8VlTR_t5h","colab_type":"code","outputId":"e370cf92-28f2-4d7e-f9d7-44b616eda46d","executionInfo":{"status":"ok","timestamp":1555652952201,"user_tz":-480,"elapsed":183624,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["sess = SparkSession.builder.appName('imdb').getOrCreate()\n","\n","pipeline = Pipeline(stages=[\n","  CountVectorizer(inputCol='words', outputCol='tf'),\n","  IDF(inputCol='tf', outputCol='tf_idf'),\n","  LogisticRegression(featuresCol='tf_idf', regParam=1.),\n","])\n","\n","df_train = make_df(X_train, y_train)\n","df_test = make_df(X_test, y_test)\n","\n","prediction = pipeline.fit(df_train).transform(df_test)\n","print(\"Testing Accuracy: {:.3f}\".format(\n","  MulticlassClassificationEvaluator(metricName='accuracy').evaluate(prediction)))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Testing Accuracy: 0.882\n"],"name":"stdout"}]}]}