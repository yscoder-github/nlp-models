{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dmn_serve.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"U6EqsJjUF0_Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"6937b9c0-f705-4f26-ae0e-81ea2908602b","executionInfo":{"status":"ok","timestamp":1550120863884,"user_tz":-480,"elapsed":1689,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}}},"cell_type":"code","source":["\"\"\"\n","We use following lines because we are running on Google Colab\n","If you are running notebook on a local computer, you don't need this cell\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir('/content/gdrive/My Drive/finch/tensorflow1/question_answering/babi/main')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"gW5Fc4DoGD9j","colab_type":"code","outputId":"87f5b252-1441-4b43-96fd-34cd43bce4a9","executionInfo":{"status":"ok","timestamp":1550120865634,"user_tz":-480,"elapsed":3411,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["import tensorflow as tf\n","print(\"TensorFlow Version\", tf.__version__)\n","print('GPU Enabled:', tf.test.is_gpu_available())\n","import numpy as np\n","import os\n","\n","from pathlib import Path\n","from attn_gru_cell import AttentionGRUCell"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow Version 1.13.0-rc1\n","GPU Enabled: True\n"],"name":"stdout"}]},{"metadata":{"id":"gYvdZVxwJTgT","colab_type":"code","colab":{}},"cell_type":"code","source":["def position_encoding(sent_size, embed_size):\n","    encoding = np.ones((embed_size, sent_size), dtype=np.float32)\n","    ls = sent_size + 1\n","    le = embed_size + 1\n","    for i in range(1, le):\n","        for j in range(1, ls):\n","            encoding[i-1, j-1] = (i - (le-1)/2) * (j - (ls-1)/2)\n","    encoding = 1 + 4 * encoding / embed_size / sent_size\n","    return tf.convert_to_tensor(np.transpose(encoding))\n","\n","  \n","def gru(rnn_size):\n","    return tf.nn.rnn_cell.GRUCell(\n","        rnn_size, kernel_initializer=tf.initializers.orthogonal())\n","\n","  \n","def embedding_module(params):\n","  return tf.get_variable('lookup', (params['vocab_size']+1, params['embed_dim']), tf.float32)\n","\n","\n","def input_module(features, vocab, embedding, params, is_training):\n","  if isinstance(features, dict):\n","    inputs = features['inputs']\n","    inputs_len = features['inputs_len']\n","  else:\n","    inputs, _, inputs_len, _ = features\n","  \n","  inputs = vocab.lookup(inputs)\n","  \n","  inputs = tf.nn.embedding_lookup(embedding, inputs)\n","  position = position_encoding(params['max_sent_len'], params['embed_dim'])\n","  inputs = tf.reduce_sum(inputs * position, 2)                          \n","  o, _ = tf.nn.bidirectional_dynamic_rnn(gru(params['hidden_size']//2),\n","                                         gru(params['hidden_size']//2),\n","                                         inputs,\n","                                         inputs_len,\n","                                         dtype=tf.float32)\n","  fact_vecs = tf.concat(o, -1)                                   \n","  fact_vecs = tf.layers.dropout(fact_vecs, params['dropout_rate'], training=is_training)\n","  return fact_vecs\n","\n","\n","def question_module(features, vocab, embedding):\n","  if isinstance(features, dict):\n","    questions = features['questions']\n","    questions_len = features['questions_len']\n","  else:\n","    _, questions, _, questions_len = features\n","  \n","  questions = vocab.lookup(questions)\n","\n","  questions = tf.nn.embedding_lookup(embedding, questions)\n","  _, state = tf.nn.dynamic_rnn(gru(params['hidden_size']),\n","                               questions,\n","                               questions_len,\n","                               dtype=tf.float32)\n","\n","  return state\n","  \n","  \n","def memory_module(features, fact_vecs, q_vec, params, is_training):\n","  proj_1 = tf.layers.Dense(params['embed_dim'], tf.tanh, name='attn_proj_1')\n","  proj_2 = tf.layers.Dense(1, name='attn_proj_2')\n","  attn_gru = AttentionGRUCell(params['hidden_size'], name='attn_gru')\n","  memory_proj = tf.layers.Dense(params['hidden_size'], tf.nn.relu, name='memory_proj')\n","\n","  memory = q_vec\n","  for i in range(params['num_hops']):\n","      print('==> Memory Episode', i+1)\n","      episode = gen_episode(features,\n","                            memory,\n","                            q_vec,\n","                            fact_vecs,\n","                            proj_1,\n","                            proj_2,\n","                            attn_gru,\n","                            is_training)\n","      memory = memory_proj(tf.concat([memory, episode, q_vec], 1))\n","\n","  return memory\n","\n","\n","def gen_episode(features, memory, q_vec, fact_vecs, proj_1, proj_2, attn_gru, is_training):\n","  def gen_attn(fact_vec):\n","      features = [fact_vec * q_vec,\n","                  fact_vec * memory,\n","                  tf.abs(fact_vec - q_vec),\n","                  tf.abs(fact_vec - memory)]\n","      feature_vec = tf.concat(features, 1)\n","      attention = proj_1(feature_vec)\n","      attention = proj_2(attention)\n","      return tf.squeeze(attention, 1)\n","\n","  if isinstance(features, dict):\n","    inputs_len = features['inputs_len']\n","  else:\n","    _, _, inputs_len, _ = features \n","    \n","  # Gates (attentions) are activated, if sentence relevant to the question or memory\n","  attns = tf.map_fn(gen_attn, tf.transpose(fact_vecs, [1,0,2]))\n","  attns = tf.transpose(attns)                                      # (B, n_fact)\n","  attns = tf.nn.softmax(attns)                                     # (B, n_fact)\n","  attns = tf.expand_dims(attns, -1)                                # (B, n_fact, 1)\n","\n","  # The relevant facts are summarized in another GRU\n","  _, episode = tf.nn.dynamic_rnn(attn_gru,\n","                                 tf.concat([fact_vecs, attns], 2), # (B, n_fact, D+1)\n","                                 inputs_len,\n","                                 dtype=tf.float32)\n","  return episode\n","\n","\n","def answer_module(memory, q_vec, params, is_training):\n","    memory = tf.layers.dropout(memory, params['dropout_rate'], training=is_training)\n","    output = tf.layers.dense(tf.concat((memory, q_vec), 1), params['vocab_size']+1)\n","    return output"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9fCnBeYCSnIt","colab_type":"code","colab":{}},"cell_type":"code","source":["def graph_fn(features, vocab, params, mode):\n","  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n","\n","  with tf.variable_scope('word_embedding'):\n","    embedding = embedding_module(params)\n","\n","  with tf.variable_scope('input_module'):\n","    fact_vecs = input_module(features, vocab, embedding, params, is_training)\n","  \n","  with tf.variable_scope('question_module'):\n","    q_vec = question_module(features, vocab, embedding)\n","\n","  with tf.variable_scope('memory_module'):\n","    memory = memory_module(features, fact_vecs, q_vec, params, is_training)\n","\n","  with tf.variable_scope('answer_module'):\n","    logits = answer_module(memory, q_vec, params, is_training)\n","\n","  return logits"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ihVShqiZSxfp","colab_type":"code","colab":{}},"cell_type":"code","source":["def model_fn(features, labels, mode, params):\n","    vocab = tf.contrib.lookup.index_table_from_file(\n","      params['vocab_path'], num_oov_buckets=1)\n","  \n","    logits = graph_fn(features, vocab, params, mode)\n","    \n","    if mode == tf.estimator.ModeKeys.PREDICT:\n","      vocab_rev = tf.contrib.lookup.index_to_string_table_from_file(params['vocab_path'])\n","      predictions = tf.argmax(logits, -1)\n","      predictions = vocab_rev.lookup(predictions)\n","      return tf.estimator.EstimatorSpec(mode=mode,\n","                                        predictions=predictions,)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"G7kqmJMB1KDT","colab_type":"code","colab":{}},"cell_type":"code","source":["params ={\n","    'model_dir': '../model/dmn',\n","    'export_dir': '../model/dmn_export',\n","    'log_path': '../log/dmn.txt',\n","    'vocab_path': '../vocab/word.txt',\n","    'max_sent_len': 6,\n","    'vocab_size': 40,\n","    'batch_size': 100,\n","    'embed_dim': 80,\n","    'hidden_size': 80,\n","    'dropout_rate': 0.1,\n","    'num_hops': 2,\n","}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"r3_A7R9a7-2H","colab_type":"code","colab":{}},"cell_type":"code","source":["def serving_input_receiver_fn():\n","    inputs = tf.placeholder(tf.string, [None, None, None], 'inputs')\n","    questions = tf.placeholder(tf.string, [None, None], 'questions')\n","    inputs_len = tf.placeholder(tf.int32, [None], 'inputs_len')\n","    questions_len = tf.placeholder(tf.int32, [None], 'questions_len')\n","    \n","    features = {'inputs': inputs,\n","                'inputs_len': inputs_len,\n","                'questions': questions,\n","                'questions_len': questions_len,}\n","    receiver_tensors = features\n","    \n","    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9KXjH1ComIB-","colab_type":"code","outputId":"ce141360-01e7-48c2-970c-53579360d486","executionInfo":{"status":"ok","timestamp":1550120868381,"user_tz":-480,"elapsed":6102,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":1008}},"cell_type":"code","source":["estimator = tf.estimator.Estimator(model_fn, params['model_dir'], params=params)\n","estimator.export_saved_model(params['export_dir'], serving_input_receiver_fn)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using default config.\n","INFO:tensorflow:Using config: {'_model_dir': '../model/dmn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe86c45c908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","INFO:tensorflow:Calling model_fn.\n","\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From <ipython-input-3-d0dff6cd0e01>:14: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-3-d0dff6cd0e01>:37: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","WARNING:tensorflow:From <ipython-input-3-d0dff6cd0e01>:39: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dropout instead.\n","==> Memory Episode 1\n","==> Memory Episode 2\n","WARNING:tensorflow:From <ipython-input-3-d0dff6cd0e01>:115: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","INFO:tensorflow:Done calling model_fn.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n","INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n","INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n","INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n","INFO:tensorflow:Signatures INCLUDED in export for Train: None\n","INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","INFO:tensorflow:Restoring parameters from ../model/dmn/model.ckpt-2001\n","INFO:tensorflow:Assets added to graph.\n","INFO:tensorflow:Assets written to: ../model/dmn_export/temp-b'1550120865'/assets\n","INFO:tensorflow:SavedModel written to: ../model/dmn_export/temp-b'1550120865'/saved_model.pb\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["b'../model/dmn_export/1550120865'"]},"metadata":{"tags":[]},"execution_count":8}]},{"metadata":{"id":"Uqfd1hoHm_Jg","colab_type":"code","outputId":"0abd9394-bfd8-4bd2-f393-eae644ebe693","executionInfo":{"status":"ok","timestamp":1550120869050,"user_tz":-480,"elapsed":6757,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"cell_type":"code","source":["inputs_example = [[\n","    ['fred', 'picked', 'up', 'the', 'football', 'there'],\n","    ['fred', 'gave', 'the', 'football', 'to', 'jev'],\n","]]\n","inputs_len_example = [2]\n","questions_example = [['what', 'did', 'fred', 'give', 'to', 'jeff']]\n","questions_len_example = [6]\n","\n","\n","subdirs = [x for x in Path(params['export_dir']).iterdir()\n","           if x.is_dir() and 'temp' not in str(x)]\n","latest = str(sorted(subdirs)[-1])\n","  \n","  \n","predict_fn = tf.contrib.predictor.from_saved_model(latest)\n","predictions = predict_fn(\n","  {'inputs': inputs_example,\n","   'inputs_len': inputs_len_example,\n","   'questions': questions_example,\n","   'questions_len': questions_len_example,})\n","\n","print('Output:', predictions['output'][0])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n","INFO:tensorflow:Restoring parameters from ../model/dmn_export/1550120865/variables/variables\n","Output: b'football'\n"],"name":"stdout"}]}]}