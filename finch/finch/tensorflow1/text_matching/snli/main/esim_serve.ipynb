{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"esim_serve.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"U6EqsJjUF0_Y","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","We use following lines because we are running on Google Colab\n","If you are running notebook on a local computer, you don't need this cell\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir('/content/gdrive/My Drive/finch/tensorflow1/text_matching/snli/main')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gW5Fc4DoGD9j","colab_type":"code","outputId":"e33761d7-7645-4032-bc43-c39f2538528c","executionInfo":{"status":"ok","timestamp":1548917780013,"user_tz":-480,"elapsed":26198,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["import tensorflow as tf\n","print(\"TensorFlow Version\", tf.__version__)\n","print('GPU Enabled:', tf.test.is_gpu_available())\n","import numpy as np\n","import os\n","\n","\n","from pathlib import Path"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow Version 1.12.0\n","GPU Enabled: True\n"],"name":"stdout"}]},{"metadata":{"id":"gYvdZVxwJTgT","colab_type":"code","colab":{}},"cell_type":"code","source":["def bi_lstm(x1, x2, seq_len_1, seq_len_2):\n","  lstm_fw = tf.contrib.rnn.LSTMBlockFusedCell(params['hidden_dim'], name='lstm_fused_fw')\n","  lstm_bw = tf.contrib.rnn.LSTMBlockFusedCell(params['hidden_dim'], name='lstm_fused_bw')\n","  lstm_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_bw)\n","  \n","  t1 = tf.transpose(x1, [1,0,2])\n","  t2 = tf.transpose(x2, [1,0,2])\n","  \n","  o1_fw, _ = lstm_fw(t1, dtype=tf.float32, sequence_length=seq_len_1)\n","  o1_bw, _ = lstm_bw(t1, dtype=tf.float32, sequence_length=seq_len_1)\n","  \n","  o2_fw, _ = lstm_fw(t2, dtype=tf.float32, sequence_length=seq_len_2)\n","  o2_bw, _ = lstm_bw(t2, dtype=tf.float32, sequence_length=seq_len_2)\n","  \n","  t1 = tf.concat([o1_fw, o1_bw], -1)\n","  t2 = tf.concat([o2_fw, o2_bw], -1)\n","  \n","  x1_ = tf.transpose(t1, [1,0,2])\n","  x2_ = tf.transpose(t2, [1,0,2])\n","  return x1_, x2_\n","\n","\n","def masked_attention(x, align, mask, tile_len):\n","  pad = tf.fill(tf.shape(align), float('-inf'))\n","  mask = tf.tile(tf.expand_dims(mask, 1), [1, tile_len, 1])\n","  align = tf.where(tf.equal(mask, 0), pad, align)\n","  align = tf.nn.softmax(align)\n","  return tf.matmul(align, x)\n","\n","  \n","def soft_align_attention(x1, x2, mask1, mask2):\n","  align12 = tf.matmul(x1, x2, transpose_b=True)\n","  align21 = tf.transpose(align12, [0,2,1])\n","  \n","  x1_ = masked_attention(x2, align12, mask2, tf.shape(x1)[1])\n","  x2_ = masked_attention(x1, align21, mask1, tf.shape(x2)[1])\n","  \n","  return x1_, x2_\n","\n","\n","def attention_pooling(x, masks, dense1, dense2, dropout, is_training):  \n","  # alignment\n","  align = dense2(dense1(x))\n","  align = tf.squeeze(align, -1)\n","  \n","  # masking\n","  paddings = tf.fill(tf.shape(align), float('-inf'))\n","  align = tf.where(tf.equal(masks, 0), paddings, align)\n","\n","  # probability\n","  align = tf.nn.softmax(align)\n","  align = dropout(align, training=is_training)\n","  align = tf.expand_dims(align, -1)\n"," \n","  # weighted sum\n","  x = tf.squeeze(tf.matmul(x, align, transpose_a=True), -1)\n","  \n","  return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KU546w9FGFL6","colab_type":"code","colab":{}},"cell_type":"code","source":["def model_fn(features, labels, mode, params):\n","  # Flag for Dropout / Batch Norm\n","  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n","  \n","  \n","  # Receive inputs\n","  if isinstance(features, dict):\n","    text1 = features['text1']\n","    text2 = features['text2']\n","  else:\n","    text1, text2 = features\n","  batch_sz = tf.shape(text1)[0]\n","  \n","  \n","  # Word Indexing\n","  vocab = tf.contrib.lookup.index_table_from_file(\n","        params['vocab_path'], num_oov_buckets=1)\n","  \n","  text1 = vocab.lookup(text1)\n","  text2 = vocab.lookup(text2)\n","  \n","  \n","  # For Masking\n","  seq_len1 = tf.count_nonzero(text1, 1, dtype=tf.int32)\n","  seq_len2 = tf.count_nonzero(text2, 1, dtype=tf.int32)\n","  \n","  mask1 = tf.sign(text1)\n","  mask2 = tf.sign(text2)\n","  \n","  \n","  # Embedding\n","  embedding = np.load(params['embedding_path'])\n","  embedding = tf.Variable(embedding, name='embedding', dtype=tf.float32)\n","  x1 = tf.nn.embedding_lookup(embedding, text1)\n","  x2 = tf.nn.embedding_lookup(embedding, text2)\n","  \n","  \n","  # Encoding\n","  dropout = tf.layers.Dropout(0.2,\n","                              noise_shape=(batch_sz, 1, params['hidden_dim']))\n","  x1 = dropout(x1, training=is_training)\n","  x2 = dropout(x2, training=is_training)\n","  \n","  x1, x2 = bi_lstm(x1, x2, seq_len1, seq_len2)\n","  \n","  \n","  # Interaction / Comparison\n","  x1_, x2_ = soft_align_attention(x1, x2, mask1, mask2)\n","  fn = lambda x, x_: tf.concat((x,\n","                                x_,\n","                                (x - x_),\n","                                (x * x_),), -1)\n","  x1 = fn(x1, x1_)\n","  x2 = fn(x2, x2_)\n","  \n","  dropout = tf.layers.Dropout(0.5,\n","                              noise_shape=(batch_sz, 1, 8*params['hidden_dim']))\n","  x1 = dropout(x1, training=is_training)\n","  x2 = dropout(x2, training=is_training)\n","  \n","  fully_connected = tf.layers.Dense(params['hidden_dim'], tf.nn.elu)\n","  x1 = fully_connected(x1)\n","  x2 = fully_connected(x2)\n","  \n","  \n","  # Encoding on top of Interaction\n","  dropout = tf.layers.Dropout(0.2,\n","                              noise_shape=(batch_sz, 1, params['hidden_dim']))\n","  x1 = dropout(x1, training=is_training)\n","  x2 = dropout(x2, training=is_training)\n","  \n","  x1, x2 = bi_lstm(x1, x2, seq_len1, seq_len2)\n","  \n","  \n","  # Extraction\n","  attn1 = tf.layers.Dense(params['hidden_dim'], tf.tanh, use_bias=False, name='attn_pool_1')\n","  attn2 = tf.layers.Dense(1, use_bias=False, name='attn_pool_2')\n","  dropout = tf.layers.Dropout(0.15)\n","  \n","  x = tf.concat((\n","      attention_pooling(x1, mask1, attn1, attn2, dropout, is_training),\n","      attention_pooling(x2, mask2, attn1, attn2, dropout, is_training),\n","      tf.reduce_max(x1, 1),\n","      tf.reduce_max(x2, 1),\n","  ), -1)\n","  \n","  \n","  # Fully Connected\n","  x = tf.layers.dropout(x, 0.5, training=is_training)\n","  \n","  x = tf.layers.dense(x, params['hidden_dim'], tf.nn.elu, name='fully_connected_1')\n","  \n","  x = tf.layers.dropout(x, 0.2, training=is_training)\n","  \n","  x = tf.layers.dense(x, params['hidden_dim'], tf.nn.elu, name='fully_connected_2')\n","  \n","  x = tf.layers.dropout(x, 0.2, training=is_training)\n","  \n","  logits = tf.layers.dense(x, params['num_labels'], name='output')\n","  \n","  \n","  if mode == tf.estimator.ModeKeys.PREDICT:\n","    return tf.estimator.EstimatorSpec(mode=mode,\n","                                      predictions=tf.argmax(logits, -1))\n","  else:\n","    loss_op = tf.nn.sparse_softmax_cross_entropy_with_logits(\n","      labels=labels, logits=logits)\n","    loss_op = tf.reduce_mean(loss_op)\n","  \n","  \n","  if mode == tf.estimator.ModeKeys.TRAIN:\n","    variables = tf.trainable_variables()\n","    tf.logging.info('\\n'+pprint.pformat(variables))\n","    \n","    grads = tf.gradients(loss_op, variables)\n","    grads, _ = tf.clip_by_global_norm(grads, params['clip_norm'])\n","    \n","    global_step=tf.train.get_or_create_global_step()\n","    decay_lr = tf.train.exponential_decay(\n","        params['lr'], global_step, 100000, .25)\n","    \n","    optim = tf.train.AdamOptimizer(decay_lr)\n","    \n","    train_op = optim.apply_gradients(\n","      zip(grads, variables), global_step=global_step)\n","    \n","    hook = tf.train.LoggingTensorHook({'lr': decay_lr}, every_n_iter=100)\n","    \n","    return tf.estimator.EstimatorSpec(mode=mode,\n","                                      loss=loss_op,\n","                                      train_op=train_op,\n","                                      training_hooks=[hook],)\n","  \n","  \n","  if mode == tf.estimator.ModeKeys.EVAL:\n","    acc_op = tf.metrics.accuracy(labels=labels,\n","                                 predictions=tf.argmax(logits, -1))\n","    \n","    return tf.estimator.EstimatorSpec(mode=mode,\n","                                      loss=loss_op,\n","                                      eval_metric_ops={'acc': acc_op})"],"execution_count":0,"outputs":[]},{"metadata":{"id":"G7kqmJMB1KDT","colab_type":"code","colab":{}},"cell_type":"code","source":["params = {\n","    'model_dir': '../model/esim',\n","    'export_dir': '../model/esim_export',\n","    'log_path': '../log/esim.txt',\n","    'train_path': '../data/train.txt',\n","    'test_path': '../data/test.txt',\n","    'embedding_path': '../vocab/word.npy',\n","    'vocab_path': '../vocab/word.txt',\n","    'batch_size': 32,\n","    'num_samples': 550152,\n","    'hidden_dim': 300,\n","    'lr': 4e-4,\n","    'clip_norm': 10.,\n","    'num_labels': 3,\n","    'num_patience': 7,\n","}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"r3_A7R9a7-2H","colab_type":"code","colab":{}},"cell_type":"code","source":["def serving_input_receiver_fn():\n","    text1 = tf.placeholder(tf.string, [None, None], 'text1')\n","    text2 = tf.placeholder(tf.string, [None, None], 'text2')\n","    \n","    features = {'text1': text1, 'text2': text2,}\n","    receiver_tensors = features\n","    \n","    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9KXjH1ComIB-","colab_type":"code","outputId":"36b9cf38-6c78-49c8-88ad-7da2bc0707c9","executionInfo":{"status":"ok","timestamp":1548917794505,"user_tz":-480,"elapsed":40662,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":436}},"cell_type":"code","source":["estimator = tf.estimator.Estimator(model_fn, params['model_dir'], params=params)\n","estimator.export_saved_model(params['export_dir'], serving_input_receiver_fn)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using default config.\n","INFO:tensorflow:Using config: {'_model_dir': '../model/esim', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3f6c500fd0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n","INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n","INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n","INFO:tensorflow:Signatures INCLUDED in export for Train: None\n","INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n","INFO:tensorflow:Restoring parameters from ../model/esim/model.ckpt-87670\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py:1044: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Pass your op to the equivalent parameter main_op instead.\n","INFO:tensorflow:Assets added to graph.\n","INFO:tensorflow:Assets written to: ../model/esim_export/temp-b'1548917780'/assets\n","INFO:tensorflow:SavedModel written to: ../model/esim_export/temp-b'1548917780'/saved_model.pb\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["b'../model/esim_export/1548917780'"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"Uqfd1hoHm_Jg","colab_type":"code","outputId":"e8916a72-1b5a-43be-98e0-5c0ce9ec87ea","executionInfo":{"status":"ok","timestamp":1548917796172,"user_tz":-480,"elapsed":42307,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/-cJ4VJthuDc0/AAAAAAAAAAI/AAAAAAAABAw/iwZyEawePbs/s64/photo.jpg","userId":"01997730851420384589"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"cell_type":"code","source":["def parse_fn(text1, text2):\n","  parse = lambda text: [[w for w in text.split()]]\n","  return {'text1': parse(text1), 'text2': parse(text2),}\n","\n","\n","TEXT1 = 'a man inspects the uniform of a figure in some east asian country'\n","TEXT2 = 'the man is sleeping'\n","idx2label = {0:'neutral', 1:'entailment', 2:'contradiction'}\n","  \n","\n","subdirs = [x for x in Path(params['export_dir']).iterdir()\n","           if x.is_dir() and 'temp' not in str(x)]\n","latest = str(sorted(subdirs)[-1])\n","  \n","  \n","predict_fn = tf.contrib.predictor.from_saved_model(latest)\n","predictions = predict_fn(parse_fn(TEXT1, TEXT2))\n","\n","\n","print('Input 1:', TEXT1)\n","print('Input 2:', TEXT2)\n","print('Output:', idx2label[predictions['output'][0]])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ../model/esim_export/1548917780/variables/variables\n","Input 1: a man inspects the uniform of a figure in some east asian country\n","Input 2: the man is sleeping\n","Output: contradiction\n"],"name":"stdout"}]}]}