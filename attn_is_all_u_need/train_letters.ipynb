{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook written by [Zhedong Zheng](https://github.com/zhedongzheng)\n",
    "\n",
    "<img src=\"transformer.png\" width=\"250\">\n",
    "\n",
    "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bunch import Bunch\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Bunch({\n",
    "    'source_max_len': 10,\n",
    "    'target_max_len': 20,\n",
    "    'min_freq': 50,\n",
    "    'hidden_units': 128,\n",
    "    'num_blocks': 2,\n",
    "    'num_heads': 8,\n",
    "    'dropout_rate': 0.1,\n",
    "    'batch_size': 64,\n",
    "    'position_encoding': 'non_param',\n",
    "    'activation': 'relu',\n",
    "    'tied_proj_weight': True,\n",
    "    'tied_embedding': True,\n",
    "    'label_smoothing': False,\n",
    "    'lr_decay_strategy': 'exp',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, source_path, target_path):\n",
    "        self.source_words = self.read_data(source_path)\n",
    "        self.target_words = self.read_data(target_path)\n",
    "\n",
    "        self.source_word2idx = self.build_index(self.source_words)\n",
    "        self.target_word2idx = self.build_index(self.target_words, is_target=True)\n",
    "\n",
    "    \n",
    "    def read_data(self, path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "\n",
    "    def build_index(self, data, is_target=False):\n",
    "        chars = [char for line in data.split('\\n') for char in line]\n",
    "        chars = [char for char, freq in Counter(chars).items() if freq > args.min_freq]\n",
    "        if is_target:\n",
    "            symbols = ['<pad>','<start>','<end>','<unk>']\n",
    "            return {char: idx for idx, char in enumerate(symbols + chars)}\n",
    "        else:\n",
    "            symbols = ['<pad>','<unk>'] if not args.tied_embedding else ['<pad>','<start>','<end>','<unk>']\n",
    "            return {char: idx for idx, char in enumerate(symbols + chars)}\n",
    "\n",
    "\n",
    "    def pad(self, data, word2idx, max_len, is_target=False):\n",
    "        res = []\n",
    "        for line in data.split('\\n'):\n",
    "            temp_line = [word2idx.get(char, word2idx['<unk>']) for char in line]\n",
    "            if len(temp_line) >= max_len:\n",
    "                if is_target:\n",
    "                    temp_line = temp_line[:(max_len-1)] + [word2idx['<end>']]\n",
    "                else:\n",
    "                    temp_line = temp_line[:max_len]\n",
    "            if len(temp_line) < max_len:\n",
    "                if is_target:\n",
    "                    temp_line += ([word2idx['<end>']] + [word2idx['<pad>']]*(max_len-len(temp_line)-1)) \n",
    "                else:\n",
    "                    temp_line += [word2idx['<pad>']] * (max_len - len(temp_line))\n",
    "            res.append(temp_line)\n",
    "        return np.array(res)\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        source_idx = self.pad(self.source_words, self.source_word2idx, args.source_max_len)\n",
    "        target_idx = self.pad(self.target_words, self.target_word2idx, args.target_max_len, is_target=True)\n",
    "        return source_idx, target_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(inputs, epsilon=1e-8):\n",
    "    mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "    normalized = (inputs - mean) / (tf.sqrt(variance + epsilon))\n",
    "\n",
    "    params_shape = inputs.get_shape()[-1:]\n",
    "    gamma = tf.get_variable('gamma', params_shape, tf.float32, tf.ones_initializer())\n",
    "    beta = tf.get_variable('beta', params_shape, tf.float32, tf.zeros_initializer())\n",
    "    \n",
    "    outputs = gamma * normalized + beta\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def embed_seq(inputs, vocab_size, embed_dim, zero_pad=False, scale=False):\n",
    "    lookup_table = tf.get_variable('lookup_table', dtype=tf.float32, shape=[vocab_size, embed_dim])\n",
    "    if zero_pad:\n",
    "        lookup_table = tf.concat((tf.zeros([1, embed_dim]), lookup_table[1:, :]), axis=0)\n",
    "    outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "    if scale:\n",
    "        outputs = outputs * np.sqrt(embed_dim)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def multihead_attn(queries, keys, q_masks, k_masks, future_binding, is_training):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q]\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k]\n",
    "    \"\"\"\n",
    "    num_units = args.hidden_units\n",
    "    num_heads = args.num_heads\n",
    "    dropout_rate = args.dropout_rate\n",
    "    \n",
    "    T_q = queries.get_shape().as_list()[1]                                         # max time length of query\n",
    "    T_k = keys.get_shape().as_list()[1]                                            # max time length of key\n",
    "\n",
    "    Q = tf.layers.dense(queries, num_units, name='Q')                              # (N, T_q, C)\n",
    "    K_V = tf.layers.dense(keys, 2*num_units, name='K_V')    \n",
    "    K, V = tf.split(K_V, 2, -1)        \n",
    "\n",
    "    Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)                         # (h*N, T_q, C/h) \n",
    "    K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)                         # (h*N, T_k, C/h) \n",
    "    V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0)                         # (h*N, T_k, C/h)\n",
    "\n",
    "    # Scaled Dot-Product\n",
    "    align = tf.matmul(Q_, tf.transpose(K_, [0,2,1]))                               # (h*N, T_q, T_k)\n",
    "    align = align / np.sqrt(K_.get_shape().as_list()[-1])                          # scale\n",
    "\n",
    "    # Key Masking\n",
    "    paddings = tf.fill(tf.shape(align), float('-inf'))                             # exp(-large) -> 0\n",
    "\n",
    "    key_masks = k_masks                                                            # (N, T_k)\n",
    "    key_masks = tf.tile(key_masks, [num_heads, 1])                                 # (h*N, T_k)\n",
    "    key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, T_q, 1])                 # (h*N, T_q, T_k)\n",
    "    align = tf.where(tf.equal(key_masks, 0), paddings, align)                      # (h*N, T_q, T_k)\n",
    "\n",
    "    if future_binding:\n",
    "        lower_tri = tf.ones([T_q, T_k])                                            # (T_q, T_k)\n",
    "        lower_tri = tf.linalg.LinearOperatorLowerTriangular(lower_tri).to_dense()  # (T_q, T_k)\n",
    "        masks = tf.tile(tf.expand_dims(lower_tri,0), [tf.shape(align)[0], 1, 1])   # (h*N, T_q, T_k)\n",
    "        align = tf.where(tf.equal(masks, 0), paddings, align)                      # (h*N, T_q, T_k)\n",
    "    \n",
    "    # Softmax\n",
    "    align = tf.nn.softmax(align)                                                   # (h*N, T_q, T_k)\n",
    "\n",
    "    # Query Masking\n",
    "    query_masks = tf.to_float(q_masks)                                             # (N, T_q)\n",
    "    query_masks = tf.tile(query_masks, [num_heads, 1])                             # (h*N, T_q)\n",
    "    query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, T_k])            # (h*N, T_q, T_k)\n",
    "    align *= query_masks                                                           # (h*N, T_q, T_k)\n",
    "\n",
    "    align = tf.layers.dropout(align, dropout_rate, training=is_training)           # (h*N, T_q, T_k)\n",
    "\n",
    "    # Weighted sum\n",
    "    outputs = tf.matmul(align, V_)                                                 # (h*N, T_q, C/h)\n",
    "    # Restore shape\n",
    "    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)              # (N, T_q, C)\n",
    "    # Residual connection\n",
    "    outputs += queries                                                             # (N, T_q, C)   \n",
    "    # Normalize\n",
    "    outputs = layer_norm(outputs)                                                  # (N, T_q, C)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def pointwise_feedforward(inputs, activation=None):\n",
    "    num_units = [4*args.hidden_units, args.hidden_units]\n",
    "    # Inner layer\n",
    "    outputs = tf.layers.conv1d(inputs, num_units[0], kernel_size=1, activation=activation)\n",
    "    # Readout layer\n",
    "    outputs = tf.layers.conv1d(outputs, num_units[1], kernel_size=1, activation=None)\n",
    "    # Residual connection\n",
    "    outputs += inputs\n",
    "    # Normalize\n",
    "    outputs = layer_norm(outputs)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def learned_position_encoding(inputs, mask, embed_dim):\n",
    "    T = inputs.get_shape().as_list()[1]\n",
    "    outputs = tf.range(tf.shape(inputs)[1])                # (T_q)\n",
    "    outputs = tf.expand_dims(outputs, 0)                   # (1, T_q)\n",
    "    outputs = tf.tile(outputs, [tf.shape(inputs)[0], 1])   # (N, T_q)\n",
    "    outputs = embed_seq(outputs, T, embed_dim, zero_pad=False, scale=False)\n",
    "    return tf.expand_dims(tf.to_float(mask), -1) * outputs\n",
    "\n",
    "\n",
    "def sinusoidal_position_encoding(inputs, mask, repr_dim):\n",
    "    T = inputs.get_shape()[1].value\n",
    "    pos = tf.reshape(tf.range(0.0, tf.to_float(T), dtype=tf.float32), [-1, 1])\n",
    "    i = np.arange(0, repr_dim, 2, np.float32)\n",
    "    denom = np.reshape(np.power(10000.0, i / repr_dim), [1, -1])\n",
    "    enc = tf.expand_dims(tf.concat([tf.sin(pos / denom), tf.cos(pos / denom)], 1), 0)\n",
    "    return tf.tile(enc, [tf.shape(inputs)[0], 1, 1]) * tf.expand_dims(tf.to_float(mask), -1)\n",
    "\n",
    "\n",
    "def label_smoothing(inputs, epsilon=0.1):\n",
    "    C = inputs.get_shape().as_list()[-1] # number of channels\n",
    "    return ((1 - epsilon) * inputs) + (epsilon / C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(sources, targets, mode, params):\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    pos_enc = _get_position_encoder()\n",
    "\n",
    "    # ENCODER\n",
    "    en_masks = tf.sign(sources)   \n",
    "\n",
    "    with tf.variable_scope('encoder_embedding'):\n",
    "        encoded = embed_seq(inputs = sources,\n",
    "                            vocab_size = params['source_vocab_size'],\n",
    "                            embed_dim = args.hidden_units,\n",
    "                            zero_pad = True,\n",
    "                            scale = True)\n",
    "\n",
    "    with tf.variable_scope('encoder_position_encoding'):\n",
    "        encoded += pos_enc(sources, en_masks, args.hidden_units)\n",
    "\n",
    "    with tf.variable_scope('encoder_dropout'):\n",
    "        encoded = tf.layers.dropout(encoded, args.dropout_rate, training=is_training)\n",
    "\n",
    "    for i in range(args.num_blocks):\n",
    "        with tf.variable_scope('encoder_attn_%d'%i):\n",
    "            encoded = multihead_attn(queries = encoded,\n",
    "                                     keys = encoded,\n",
    "                                     q_masks = en_masks,\n",
    "                                     k_masks = en_masks,\n",
    "                                     future_binding = False,\n",
    "                                     is_training = is_training,)\n",
    "\n",
    "        with tf.variable_scope('encoder_feedforward_%d'%i):\n",
    "            encoded = pointwise_feedforward(encoded,\n",
    "                                            activation = params['activation'])\n",
    "\n",
    "    # DECODER\n",
    "    decoder_inputs = _shift_right(targets, params['start_symbol'])\n",
    "    de_masks = tf.sign(decoder_inputs)\n",
    "\n",
    "    if args.tied_embedding:\n",
    "        vs = tf.variable_scope('encoder_embedding', reuse=True)\n",
    "    else:\n",
    "        vs = tf.variable_scope('decoder_embedding')\n",
    "    with vs:\n",
    "        decoded = embed_seq(decoder_inputs,\n",
    "                            params['target_vocab_size'],\n",
    "                            args.hidden_units,\n",
    "                            zero_pad = True,\n",
    "                            scale = True)\n",
    "\n",
    "    with tf.variable_scope('decoder_position_encoding'):\n",
    "        decoded += pos_enc(decoder_inputs, de_masks, args.hidden_units)\n",
    "\n",
    "    with tf.variable_scope('decoder_dropout'):\n",
    "        decoded = tf.layers.dropout(decoded, args.dropout_rate, training=is_training)\n",
    "\n",
    "    for i in range(args.num_blocks):\n",
    "        with tf.variable_scope('decoder_self_attn_%d'%i):\n",
    "            decoded = multihead_attn(queries = decoded,\n",
    "                                     keys = decoded,\n",
    "                                     q_masks = de_masks,\n",
    "                                     k_masks = de_masks,\n",
    "                                     future_binding = True,\n",
    "                                     is_training = is_training)\n",
    "\n",
    "        with tf.variable_scope('decoder_attn_%d'%i):\n",
    "            decoded = multihead_attn(queries=decoded,\n",
    "                                     keys = encoded,\n",
    "                                     q_masks = de_masks,\n",
    "                                     k_masks = en_masks,\n",
    "                                     future_binding = False,\n",
    "                                     is_training = is_training)\n",
    "\n",
    "        with tf.variable_scope('decoder_feedforward_%d'%i):\n",
    "            decoded = pointwise_feedforward(decoded,\n",
    "                                            activation = params['activation'])\n",
    "\n",
    "    # OUTPUT LAYER    \n",
    "    if args.tied_proj_weight:\n",
    "        b = tf.get_variable('bias', [params['target_vocab_size']], tf.float32)\n",
    "        _scope = 'encoder_embedding' if args.tied_embedding else 'decoder_embedding'\n",
    "        with tf.variable_scope(_scope, reuse=True):\n",
    "            shared_w = tf.get_variable('lookup_table')\n",
    "        decoded = tf.reshape(decoded, [-1, args.hidden_units])\n",
    "        logits = tf.nn.xw_plus_b(decoded, tf.transpose(shared_w), b)\n",
    "        logits = tf.reshape(logits, [tf.shape(sources)[0], -1, params['target_vocab_size']])\n",
    "    else:\n",
    "        with tf.variable_scope('output_layer'):\n",
    "            logits = tf.layers.dense(decoded, params['target_vocab_size'], reuse=reuse)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def _model_fn_train(features, mode, params):\n",
    "    logits = forward_pass(features['source'], features['target'], mode, params)\n",
    "    targets = features['target']\n",
    "    masks = tf.to_float(tf.not_equal(targets, 0))\n",
    "\n",
    "    if args.label_smoothing:\n",
    "        loss_op = label_smoothing_sequence_loss(logits = logits,\n",
    "                                                targets = targets,\n",
    "                                                weights = masks,\n",
    "                                                label_depth = params['target_vocab_size'])\n",
    "    else:\n",
    "        loss_op = tf.contrib.seq2seq.sequence_loss(logits = logits,\n",
    "                                                   targets = targets,\n",
    "                                                   weights = masks)\n",
    "\n",
    "    if args.lr_decay_strategy == 'noam':\n",
    "        step_num = tf.train.get_global_step() + 1   # prevents zero global step\n",
    "        lr = _get_noam_lr(step_num)\n",
    "    elif args.lr_decay_strategy == 'exp':\n",
    "        lr = tf.train.exponential_decay(1e-3, tf.train.get_global_step(), 100000, 0.1)\n",
    "    else:\n",
    "        raise ValueError(\"lr decay strategy must be one of 'noam' and 'exp'\")\n",
    "    log_hook = tf.train.LoggingTensorHook({'lr': lr}, every_n_iter=100)\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(loss_op,\n",
    "                                                   global_step = tf.train.get_global_step())\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode = mode,\n",
    "                                      loss = loss_op,\n",
    "                                      train_op = train_op,\n",
    "                                      training_hooks = [log_hook])\n",
    "\n",
    "\n",
    "def _model_fn_predict(features, mode, params):\n",
    "    def cond(i, x, temp):\n",
    "        return i < args.target_max_len\n",
    "\n",
    "    def body(i, x, temp):\n",
    "        logits = forward_pass(features['source'], x, mode, params)\n",
    "        ids = tf.argmax(logits, -1)[:, i]\n",
    "        ids = tf.expand_dims(ids, -1)\n",
    "\n",
    "        temp = tf.concat([temp[:, 1:], ids], -1)\n",
    "\n",
    "        x = tf.concat([temp[:, -(i+1):], temp[:, :-(i+1)]], -1)\n",
    "        x = tf.reshape(x, [tf.shape(temp)[0], args.target_max_len])\n",
    "        i += 1\n",
    "        return i, x, temp\n",
    "\n",
    "    _, res, _ = tf.while_loop(cond, body, [tf.constant(0), features['target'], features['target']])\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=res)\n",
    "\n",
    "\n",
    "def tf_estimator_model_fn(features, labels, mode, params):\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return _model_fn_train(features, mode, params)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return _model_fn_predict(features, mode, params)\n",
    "\n",
    "\n",
    "def _shift_right(targets, start_symbol):\n",
    "    start_symbols = tf.cast(tf.fill([tf.shape(targets)[0], 1], start_symbol), tf.int64)\n",
    "    return tf.concat([start_symbols, targets[:, :-1]], axis=-1)\n",
    "\n",
    "\n",
    "def _get_position_encoder():\n",
    "    if args.position_encoding == 'non_param':\n",
    "        pos_enc = sinusoidal_position_encoding\n",
    "    elif args.position_encoding == 'param':\n",
    "        pos_enc = learned_position_encoding\n",
    "    else:\n",
    "        raise ValueError(\"position encoding has to be either 'param' or 'non_param'\")\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "def _get_noam_lr(step_num):\n",
    "    return tf.rsqrt(tf.to_float(args.hidden_units)) * tf.minimum(\n",
    "        tf.rsqrt(tf.to_float(step_num)),\n",
    "        tf.to_float(step_num) * tf.convert_to_tensor(args.warmup_steps ** (-1.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"source_max_len\": 10,\n",
      "    \"target_max_len\": 20,\n",
      "    \"min_freq\": 50,\n",
      "    \"hidden_units\": 128,\n",
      "    \"num_blocks\": 2,\n",
      "    \"num_heads\": 8,\n",
      "    \"dropout_rate\": 0.1,\n",
      "    \"batch_size\": 64,\n",
      "    \"position_encoding\": \"non_param\",\n",
      "    \"activation\": \"relu\",\n",
      "    \"tied_proj_weight\": true,\n",
      "    \"tied_embedding\": true,\n",
      "    \"label_smoothing\": false,\n",
      "    \"lr_decay_strategy\": \"exp\"\n",
      "}\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp5khqlnbl\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp5khqlnbl', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x117d8c978>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp5khqlnbl/model.ckpt.\n",
      "INFO:tensorflow:loss = 4.5672035, step = 1\n",
      "INFO:tensorflow:lr = 0.001\n",
      "INFO:tensorflow:global_step/sec: 4.56219\n",
      "INFO:tensorflow:loss = 0.35741818, step = 101 (21.921 sec)\n",
      "INFO:tensorflow:lr = 0.0009977 (21.921 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.04297\n",
      "INFO:tensorflow:loss = 0.28123516, step = 201 (19.830 sec)\n",
      "INFO:tensorflow:lr = 0.0009954055 (19.830 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.05076\n",
      "INFO:tensorflow:loss = 0.19661611, step = 301 (19.799 sec)\n",
      "INFO:tensorflow:lr = 0.0009931161 (19.799 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.11878\n",
      "INFO:tensorflow:loss = 0.12202566, step = 401 (19.536 sec)\n",
      "INFO:tensorflow:lr = 0.000990832 (19.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.74806\n",
      "INFO:tensorflow:loss = 0.15006474, step = 501 (21.062 sec)\n",
      "INFO:tensorflow:lr = 0.0009885532 (21.062 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.74353\n",
      "INFO:tensorflow:loss = 0.1697518, step = 601 (21.081 sec)\n",
      "INFO:tensorflow:lr = 0.0009862796 (21.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.05783\n",
      "INFO:tensorflow:loss = 0.13487265, step = 701 (19.771 sec)\n",
      "INFO:tensorflow:lr = 0.0009840112 (19.772 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.19017\n",
      "INFO:tensorflow:loss = 0.06525661, step = 801 (19.267 sec)\n",
      "INFO:tensorflow:lr = 0.000981748 (19.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.8692\n",
      "INFO:tensorflow:loss = 0.10412187, step = 901 (20.537 sec)\n",
      "INFO:tensorflow:lr = 0.00097949 (20.538 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp5khqlnbl/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0569329.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp5khqlnbl/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "apple -> aelpp\n",
      "common -> cmmnoo\n",
      "zhedong -> deghnoz\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp5khqlnbl/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "def greedy_decode(test_words, tf_estimator, dl):\n",
    "    test_indices = []\n",
    "    for test_word in test_words:\n",
    "        test_idx = [dl.source_word2idx[c] for c in test_word] + \\\n",
    "                   [dl.source_word2idx['<pad>']] * (args.source_max_len - len(test_word))\n",
    "        test_indices.append(test_idx)\n",
    "    test_indices = np.atleast_2d(test_indices)\n",
    "    \n",
    "    zeros = np.zeros([len(test_words), args.target_max_len], np.int64)\n",
    "\n",
    "    pred_ids = tf_estimator.predict(tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'source':test_indices, 'target':zeros}, batch_size=len(test_words), shuffle=False))\n",
    "    pred_ids = list(pred_ids)\n",
    "    \n",
    "    target_idx2word = {i: w for w, i in dl.target_word2idx.items()}\n",
    "    for i, test_word in enumerate(test_words):\n",
    "        ans = ''.join([target_idx2word[id] for id in pred_ids[i]])\n",
    "        print(test_word, '->', ans.replace('<end>', ''))\n",
    "\n",
    "\n",
    "def prepare_params(dl):\n",
    "    if args.activation == 'relu':\n",
    "        activation = tf.nn.relu\n",
    "    elif args.activation == 'elu':\n",
    "        activation = tf.nn.elu\n",
    "    elif args.activation == 'lrelu':\n",
    "        activation = tf.nn.leaky_relu\n",
    "    else:\n",
    "        raise ValueError(\"acitivation fn has to be 'relu' or 'elu' or 'lrelu'\")\n",
    "    params = {\n",
    "        'source_vocab_size': len(dl.source_word2idx),\n",
    "        'target_vocab_size': len(dl.target_word2idx),\n",
    "        'start_symbol': dl.target_word2idx['<start>'],\n",
    "        'activation': activation}\n",
    "    return params\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(json.dumps(args, indent=4))\n",
    "    \n",
    "    dl = DataLoader(\n",
    "        source_path='../temp/letters_source.txt',\n",
    "        target_path='../temp/letters_target.txt')\n",
    "    sources, targets = dl.load()\n",
    "    \n",
    "    tf_estimator = tf.estimator.Estimator(\n",
    "        tf_estimator_model_fn, params=prepare_params(dl))\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        tf_estimator.train(tf.estimator.inputs.numpy_input_fn(\n",
    "            x = {'source':sources, 'target':targets},\n",
    "            batch_size = args.batch_size,\n",
    "            num_epochs = None,\n",
    "            shuffle = True), steps=1000)\n",
    "        greedy_decode(['apple', 'common', 'zhedong'], tf_estimator, dl)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
